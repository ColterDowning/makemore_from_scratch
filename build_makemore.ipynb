{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to make a new name generator =) We will start with a list of names and create an autoregressive character-level language model that generates names that sound like\n",
    "# the names we are given. \n",
    "\n",
    "#Start with importing Pytorch and matplotlib and placing the names.txt file in the same folder as this notebook\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initializes a dictionary b that we can use to see some statistics about all the names. We will iterate over all two character possibilites in the names and make them a dictionary key. \n",
    "# The values increase by one when we iterate over an example of the two character pair\n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1,ch2)\n",
    "        b[bigram] = b.get(bigram,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see the most likely two character pairs. An 'n' followed by the end of the name appears the most often. An 'a' followed by an 'n' happens the third most often.\n",
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dictionaries, Pytorch likes arrays (or tensors)\n",
    "N = torch.zeros((27,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes all the names in the .txt file, conncatenates into one massive string, throws out duplicates (with the set function), puts them in a list, and sorts them. \n",
    "# The result is the alphabet =) We call this chars\n",
    "\n",
    "#stoi is the mapping of each string to an integer. This assigns a number to each letter\n",
    "#itos is the inverse of stoi\n",
    "#Instead of having special characters for the start and end of a name, we will just use the character '.' to represent both\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same method from above, but adapted for Pytorch. Remember, we want to start by understanding some statistics about the names we are given. In particular, which character (or letter) are you\n",
    "# most likely to get given a letter\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # This is a cute way to get a list of tuples where each tuple is the letter and the letter that follows it.\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize N to get a better understanding. Here we see all possible 2 character combos, with the number of times they appear. The more they appear, the more blue the box becomes.\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i,j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok this is great, we can now see which 2 letter pairs are most common. That means if I give you 1 letter, you can tell me which letter is most likely to follow. To make new names that\n",
    "# sound real, we want to choose these most likely options. \n",
    "\n",
    "# Now how do we work with this distribution? We need to build a way to sample from this distribution.\n",
    "\n",
    "#This is the first row of the matrix above. It is equivalent to N[0, :].\n",
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This turns the first row into floats, then normalizes over the row. This is our probability distribution for the question \"which character is most likely to start a name?\"\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a distribution, we can take a sample from it using Multimodial(). In this example, our character is 'J'. This represents the first character generation for a new name!\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a better understanding of Multimodal, see the example below. \n",
    "\n",
    "#g is a generator object that is useful for being deterministic, since we can use seeds. p then generates 3 random number using the g generator, and we normalize.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal is a way to draw samples from a distribution. It will take a torch tensor probability distribution and draw a number of samples num_samples. Notice that there \n",
    "# are mostly 0s, some 1s, and few 2s. This is because the likelihood of pulling a 0 is ~60%, the liklihood of pulling a 1 is ~30%, and a 2 is ~9%. This is the distribution of tensor p above.\n",
    "torch.multinomial(p, num_samples=20, replacement=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "# We have our first character. Going back to the visualized matrix above, we can take the first character and use it as a lookup. We can ask \"Given this first character, what character\n",
    "# is most likely to come after it?\" As you can probably guess, this will become a pattern of using a generated letter to feed back into the generator to get a new letter. Let's write\n",
    "# out this loop.\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) #Create your generator\n",
    "P = (N+1).float() # N + 1 is model smoothing. The more you add, the more uniform your distribution will be. Makes it so there are no 0s in the blue matrix above.\n",
    "P /= P.sum(1, keepdim=True) #Broadcasting here is incredibly tricky. Pay attention to it, and respect it. Check your work. This is also an in-place operation (/=), which doesn't create a new\n",
    "# vector, and therefore saves memory.\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0 # Start at the first index\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        #p = torch.ones(27) / 27.0\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() #Draw a sample from the distribution. This will serve as the next character to feed into the generator\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0: #break case where our sampled letter is the 'end' token, meaning the end of the name\n",
    "            break\n",
    "    print(''.join(out))\n",
    "\n",
    "#The name results are pretty bad lol =) But, its reasonable to assume these results are 'better' than random selection. Uncomment the #p = torch.ones(27) / 27.0 and comment the P above\n",
    "# the while loop. This represents a uniform distribution, so the generator is equally likely to choose any character. Those results are much worse than the p distribution trained on \n",
    "# the bigram list of names. Bigram is still bad though =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-559951.5625)\n",
      "nll=tensor(559951.5625)\n",
      "2.4543561935424805\n"
     ]
    }
   ],
   "source": [
    "# Great, so we now have names, but they look bad to our eye. How do we evaluate the quality of this model? We will come back to our old friend the loss function. =) This will give us a \n",
    "# single number that assesses the quality of the model. The lower the number, the better the model. The autograd repository describes the loss function in more detail, but we are essentially\n",
    "# measuring the difference between the output of our model and what we want the model to ouput. If that difference is 0, then our model outputs exactly what we want =)\n",
    "# A commonly used loss function is something called the 'likelihood function'. \n",
    "\n",
    "# Consider the following: I give you 1) a set of parameters and 2) some observed data. Assume there exists some underlying probability distribution. The Maximum Likelihood Estimation \n",
    "# is a method for estimating the parameters of the assumed probability distribution in such a way that makes the observed data the most likely. This is achieved by maximizing \n",
    "# the 'likelihood function'.\n",
    "\n",
    "# https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1\n",
    "\n",
    "# In our case:\n",
    "# The model is N, the counts of each bigram.\n",
    "# the model parameters are defined in the blue matrix above (the bigram probabilities)\n",
    "# the probability distribution is P\n",
    "# the data is words (names.txt)\n",
    "# The input data is the first letter of the bigram, the desired output is the second letter of the bigram\n",
    "\n",
    "# Using MLE, what set of parameters (bigram probabilities of blue matrix) for the distribution P is most likely resposible for producing the data in names.txt\n",
    "\n",
    "# Andrej Karpathy explains the following:\n",
    "# GOAL: maximize likelihood of the data with respect to model parameters\n",
    "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "# equivalent to minimizing the negative log likelihood\n",
    "# equivalent to minimizing the average negative log likelihood  \n",
    " \n",
    "# Let's create this likelihood function\n",
    "\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # This is a cute way to get a list of tuples where each tuple is the letter and the letter that follows it.\n",
    "        ix1 = stoi[ch1] #input to the model\n",
    "        ix2 = stoi[ch2] #desired output\n",
    "        prob = P[ix1, ix2] #probability that the model assigns to this happening\n",
    "        logprob = torch.log(prob) # log is a monotonic function, so we can use it to scale our function. It is more computationally efficient to do this because we can add instead of multiply\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood # nll is negative log likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}') # The lower this number, the better. nll/n is our loss function.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up until now, we have arrived at a model explicitly by doing things that \"feel correct\", meaning measuring and normalizing counts. Now we will take an alternative approach that\n",
    "# will arrive at a similar outcome. We will cast the bigram character level language modeling into the neural network framework. The neural network will still be a bigram character level model,\n",
    "# meaning it will receive a character as an input, then with the weights and biases of the network will output the probability distribution of the next character. It will makes guesses as\n",
    "# to what is likely to follow the input character.\n",
    "\n",
    "# The first thing we want to do is create the training set for the network. This code iterates over all the bigrams (x,y)\n",
    "xs, ys = [], [] #xs are the inputs, ys are the targets\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1,ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # use lower case .tensor to get integers. capital T .Tensor will give float32.\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "# In this example with the name 'emma', there are 5 pieces of information. When '.' is the input, 'e' is the target. Put in terms of indexes, when 0 is the input 5 is the desired output.\n",
    "# When 5 is the input 13 is the desired output, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context of a neural network, it doesn't make sense to feed in integers. The network is created with nodes that are multiplied by weights and scaled with biases. Think of training\n",
    "# the network as tuning a vector space to be aligned with a desired outcome. We want to do this by working with vectors, not integers. Fortunatley, Pytorch has an easy way of dealing with \n",
    "# this called one hot. This takes the integer and turns the integer's index dimension into a 1. The rest are 0s. For example, an integer of 5 would result in a vector of all 0s except that\n",
    "# the 5th dimension of the vector is a 1.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape # 5x27 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xenc) # This is the picture of the name emma =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make our first neuron.\n",
    "W = torch.randn((27,1)) # W is a column vector of 27 weights\n",
    "xenc @ W # The @ symbol is the matrix multiplication operator in pytorch. So cool!\n",
    "\n",
    "# Note that the resulting dot product is a 5 x 1 matrix. We took xenc which is 5 x 27 and multiplied by 27 x 1. \n",
    "# We fed all 5 inputs into a single neuron, then calculated the output with matrix multiplication of x @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now create the 27 weights for each neuron, but also create 27 neurons. \n",
    "W = torch.randn((27,27)) \n",
    "xenc @ W\n",
    "\n",
    "# (5, 27) @ (27, 27) -> (5, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the firing rate of the 13th neuron, looking at the 3rd input\n",
    "(xenc @ W)[3, 13]\n",
    "\n",
    "# Same as (xenc[3] * W[:, 13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a neural network with 27 inputs, and 27 neurons in the first layer, with only linear scaling (no bias, no squishing funciton like tanh, no other layers). Very simple.\n",
    "# But our neuron values are these small float values. We want something like the original 2-dim blue matrix N. Each row told us the counts, and we can normalize each row\n",
    "# to get the probabilities. However, counts aren't great to output from a neural net either. Let's instead interpret the output as log(counts).\n",
    "\n",
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() #equivalent N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs # Each row is an output from the neural net (from 1 input) and the columns are the probabilities for each next letter\n",
    "\n",
    "# Note that each operation here is differentiable, so we can back-propogate through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0] # This is the input of a '.' into the neural net. We first got its index, then one hot encoded it, put into the neural net, and the output is the following 27 numbers. These\n",
    "# numbers represent how likely each character is to come next. As we tune the weights W, we will get different probabilities out. Now the question is can we tune W so that the probabilties\n",
    "# coming out are good. The way we measure good is by the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY ------------------------------>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represents the feed forward\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = torch.zeros(5) # 5 bigrams in the name 'emma'\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- !!! OPTIMIZATION !!! yay --------------\n",
    "\n",
    "# Just like in the autograd engine, we are going to optimize the neural network through gradient descent. \n",
    "# For more info, check https://github.com/ColterDowning/Neural-Networks-Zero-to-Hero/tree/master/micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6891887187957764\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass. First make sure to reset the gradients\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- !!! OPTIMIZATION !!! yay, but this time actually --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.768618583679199\n",
      "3.3788068294525146\n",
      "3.161090850830078\n",
      "3.0271859169006348\n",
      "2.9344842433929443\n",
      "2.867231607437134\n",
      "2.8166542053222656\n",
      "2.777146577835083\n",
      "2.745253801345825\n",
      "2.7188305854797363\n",
      "2.696505308151245\n",
      "2.6773719787597656\n",
      "2.6608052253723145\n",
      "2.6463515758514404\n",
      "2.633665084838867\n",
      "2.622471570968628\n",
      "2.6125476360321045\n",
      "2.6037068367004395\n",
      "2.595794916152954\n",
      "2.5886807441711426\n",
      "2.5822560787200928\n",
      "2.576429843902588\n",
      "2.5711236000061035\n",
      "2.566272735595703\n",
      "2.5618226528167725\n",
      "2.5577261447906494\n",
      "2.5539441108703613\n",
      "2.550442695617676\n",
      "2.5471930503845215\n",
      "2.5441699028015137\n",
      "2.5413522720336914\n",
      "2.538722038269043\n",
      "2.536262035369873\n",
      "2.5339579582214355\n",
      "2.531797409057617\n",
      "2.529768228530884\n",
      "2.527860164642334\n",
      "2.5260636806488037\n",
      "2.5243704319000244\n",
      "2.522773265838623\n",
      "2.52126407623291\n",
      "2.519836664199829\n",
      "2.5184857845306396\n",
      "2.5172054767608643\n",
      "2.515990734100342\n",
      "2.5148372650146484\n",
      "2.5137407779693604\n",
      "2.512697696685791\n",
      "2.511704921722412\n",
      "2.5107579231262207\n",
      "2.509855031967163\n",
      "2.5089924335479736\n",
      "2.5081679821014404\n",
      "2.507380485534668\n",
      "2.5066258907318115\n",
      "2.5059030055999756\n",
      "2.5052103996276855\n",
      "2.5045459270477295\n",
      "2.503908157348633\n",
      "2.503295421600342\n",
      "2.5027060508728027\n",
      "2.5021398067474365\n",
      "2.501594305038452\n",
      "2.5010695457458496\n",
      "2.500563383102417\n",
      "2.500075578689575\n",
      "2.4996049404144287\n",
      "2.499150514602661\n",
      "2.4987120628356934\n",
      "2.49828839302063\n",
      "2.4978787899017334\n",
      "2.4974827766418457\n",
      "2.4970996379852295\n",
      "2.4967293739318848\n",
      "2.496370315551758\n",
      "2.4960227012634277\n",
      "2.4956860542297363\n",
      "2.4953596591949463\n",
      "2.4950432777404785\n",
      "2.494736433029175\n",
      "2.494438886642456\n",
      "2.494149684906006\n",
      "2.4938690662384033\n",
      "2.4935965538024902\n",
      "2.4933321475982666\n",
      "2.493075132369995\n",
      "2.4928252696990967\n",
      "2.492582321166992\n",
      "2.4923462867736816\n",
      "2.492116689682007\n",
      "2.4918932914733887\n",
      "2.491675853729248\n",
      "2.491464376449585\n",
      "2.491258382797241\n",
      "2.491057872772217\n",
      "2.4908623695373535\n",
      "2.4906723499298096\n",
      "2.4904870986938477\n",
      "2.4903063774108887\n",
      "2.4901304244995117\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "\n",
    "# Note that the optimized loss function is about the same as the loss function from the original counting and MLE method (2.45 vs 2.49). This makes sense since we aren't taking in any\n",
    "# new information. We are still just taking in a character and predicting the next one. But now, instead of just explicitly counting and normalizing, we are using gradient based learning.\n",
    "# The original blue matrix is essentially the same as the fully optimized W.exp() in the gradient approach. We started with a random set and let the loss guide us to the blue matrix.\n",
    "\n",
    "# It just so happens that the explicit approach optimizes the loss function because the setup is so simple. We can just estimate those probabilties directly and maintain in a table.\n",
    "# The gradient based approach is way more flexible. We can easily scale to input multiple characters to predict the next character and complexify the net, while still outputting a single logit.\n",
    "# It's not obvious how we would have extended the bigram approach to include multiple character inputs. The tables would have gotten way too large to handle because there are way too\n",
    "# many possible combinations (exponential).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
